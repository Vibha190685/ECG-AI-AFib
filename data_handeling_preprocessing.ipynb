{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import utility_new as pc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Dropout, Activation, BatchNormalization, Add\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPool1D, ZeroPadding1D, LSTM, Bidirectional\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "import ecg_plot\n",
    "import heartpy as hp\n",
    "\n",
    "import pywt\n",
    "import heartpy as hp\n",
    "\n",
    "from itables import init_notebook_mode\n",
    "\n",
    "init_notebook_mode(all_interactive=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "%reload_ext autoreload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_available_classes=['270492004','164889003', '164890007', '426627000','713427006', '713426002','445118002', '39732003','164909002', '251146004','698252002', '10370003','284470004','427172004','164947007', '111975006','164917005', '47665007','59118001', '427393009','426177001', '426783006','427084000', '63593006','164934002', '59931005', '17338001']\n",
    "path_G=\"C:/Users/Admin/Downloads/archive/G12ECG/WFDB/\"\n",
    "path_P=\"C:/Users/Admin/Downloads/archive/PTB_XL/WFDB/\"\n",
    "path_C=\"C:/Users/Admin/Downloads/archive/CPSC_Extra/\"\n",
    "positive_classes  = ['164889003', '164890007']\n",
    "dataset_paths = [path_C, path_G, path_P]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##To check signal count for positive and negative  classes\n",
    "positive_classes  = ['164889003', '164890007']\n",
    "gender, age, labels, ecg_filenames, class_counts, samples_with_positive_class=pc.import_key_data_with_simple(path_P, positive_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##function for wavelet denosing and HRV extrcation\n",
    "# Filter with sym5\n",
    "def sym5_wavelet_filter(signal):\n",
    "    # Apply sym5 wavelet filter\n",
    "    coeffs = pywt.wavedec(signal, 'sym5', level=9)\n",
    "    coeffs[1:] = (pywt.threshold(c, 0.1, mode='soft') for c in coeffs[1:])\n",
    "    reconstructed_signal = pywt.waverec(coeffs, 'sym5')\n",
    "    return reconstructed_signal\n",
    "\n",
    "# Try to calculate the heart rate across all leads, across all files\n",
    "def heartrate_coll(all_ecg_filenames):\n",
    "    # Create a list to store rows\n",
    "    rows = []\n",
    "\n",
    "    for index in range(len(all_ecg_filenames)):\n",
    "        try:\n",
    "            data, header = pc.load_challenge_data(all_ecg_filenames[index])\n",
    "\n",
    "            # Initialize a list to store heart rates for each lead\n",
    "            heart_rates = [index]\n",
    "\n",
    "            # Calculate heart rate for each lead\n",
    "            for lead_num in range(12):\n",
    "                try:\n",
    "                    # Apply sym5 wavelet filter\n",
    "                    filtered_ecg = sym5_wavelet_filter(data[lead_num])\n",
    "\n",
    "                    # Calculate heart rate\n",
    "                    wd, m = hp.process(filtered_ecg, sample_rate=500)\n",
    "                    heart_rate = m['bpm']\n",
    "                    heart_rates.append(heart_rate)\n",
    "                except Exception as e:\n",
    "                    heart_rates.append(np.nan)\n",
    "\n",
    "            # Append the heart rates to the list\n",
    "            rows.append(heart_rates)\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Processed index {index}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing index {index}: {str(e)}\")\n",
    "\n",
    "    # Create a DataFrame from the list of rows\n",
    "    columns = ['Index'] + [f'Lead_{i}' for i in range(12)]\n",
    "    heart_rates_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    # Calculate median and add a new column\n",
    "    heart_rates_df['Median_HeartRate'] = heart_rates_df.iloc[:, 1:].apply(lambda x: np.nanmedian(x), axis=1)\n",
    "\n",
    "    # Add lead names column\n",
    "    lead_names = [f'Lead_{i}' for i in range(12)]\n",
    "    heart_rates_df['Lead_Name'] = heart_rates_df.apply(lambda x: get_lead_name(x,lead_names), axis=1)\n",
    "\n",
    "    return heart_rates_df\n",
    "\n",
    "def get_lead_name(row,lead_names):\n",
    "     # Filter out NaN values and find the lead with the closest value to the median\n",
    "    closest_lead = min(lead_names, key=lambda lead: abs(row[lead] - row['Median_HeartRate']) if not np.isnan(row[lead]) else np.inf)\n",
    "\n",
    "    return closest_lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_rates_df = heartrate_coll(all_ecg_filenames)\n",
    "heart_rates_df.to_csv('heart_rates_with_all_lead_new.csv', index=False)\n",
    "print(heart_rates_df)\n",
    "\n",
    "heart_rates_df_selected = heart_rates_df[\n",
    "    (~heart_rates_df['Lead_Name'].isna()) &  # Exclude rows where 'Lead_Name' is NaN\n",
    "    (~heart_rates_df['Median_HeartRate'].isna()) &  # Exclude rows where 'Median_HeartRate' is NaN\n",
    "\n",
    "]\n",
    "\n",
    "len(heart_rates_df_selected )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(file_path, lead):\n",
    "    # Load ECG data from the MAT file (replace this with your actual method)\n",
    "    mat_data = pc.load_challenge_data(file_path)[0]\n",
    "    \n",
    "    # Use HeartPy to process the lead data and calculate metrics\n",
    "    data, measures = hp.process(mat_data[lead], sample_rate=500.0)  # Adjust sample_rate as needed\n",
    "    \n",
    "    # Access the calculated metrics\n",
    "    heart_rate_parameters = {\n",
    "        'HeartRate': measures['bpm'],# Beats Per Minute, representing the heart rate.\n",
    "        'InterBeatInterval': measures['ibi'],# Inter-Beat Interval, the time between successive heartbeats.\n",
    "        'HRV_SDNN': measures['sdnn'],#Standard Deviation of NN intervals, a measure of heart rate variability.\n",
    "        'HRV_SDSD': measures['sdsd'],#Standard Deviation of successive differences between NN intervals.\n",
    "        'HRV_RMSSD': measures['rmssd'],# Root Mean Square of Successive Differences between NN intervals.\n",
    "        'PNN20': measures['pnn20'],#Percentage of successive NN intervals differing by more than 20 milliseconds.\n",
    "        'PNN50': measures['pnn50'],# Percentage of successive NN intervals differing by more than 50 milliseconds.\n",
    "        'HR_MAD': measures['hr_mad'],#Heart Rate Mean Absolute Deviation.\n",
    "        'Ratio_of_SD1_SD2': measures['s'],#A parameter without specific information in the given context.\n",
    "        'InfoS': measures['hr_mad']#Heart Rate Mean Absolute Deviation.\n",
    "        # Add more metrics as needed\n",
    "    }\n",
    "\n",
    "    return heart_rate_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the calculated metrics\n",
    "calculated_metrics_df = pd.DataFrame()\n",
    "# Iterate through each row in the original DataFrame and calculate metrics\n",
    "for index in range(len(selected_ecg_df)):\n",
    "    row = selected_ecg_df.iloc[index, :]\n",
    "    file_path = row['Filename']\n",
    "    lead = pd.to_numeric(row['LEAD'])\n",
    "    age = row['Age']\n",
    "    gender = row['Gender']\n",
    "    label=row['labels']\n",
    "    # Calculate metrics for the current file and lead\n",
    "    heart_rate_measures = calculate_metrics(file_path, lead)\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    metrics_df = pd.DataFrame([heart_rate_measures])\n",
    "    \n",
    "    # Include additional columns in the DataFrame\n",
    "    metrics_df['Age'] = age\n",
    "    metrics_df['Gender'] = gender\n",
    "    metrics_df['Encoded_labels'] = label\n",
    "    metrics_df['Filename'] = file_path\n",
    "    metrics_df['LEAD'] = lead\n",
    "    \n",
    "    # Concatenate the DataFrame with calculated metrics to the main DataFrame\n",
    "    calculated_metrics_df = pd.concat([calculated_metrics_df, metrics_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_metrics_df.to_csv('Final_frame_to_work_withFilterSignalLengthCheck_new.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
