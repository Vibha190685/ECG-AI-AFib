{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "import utility_new as pc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Dropout, Activation, BatchNormalization, Add\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPool1D, ZeroPadding1D, LSTM, Bidirectional\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "import ecg_plot\n",
    "import heartpy as hp\n",
    "import pywt\n",
    "import utility_new as pc\n",
    "\n",
    "from itables import init_notebook_mode\n",
    "\n",
    "init_notebook_mode(all_interactive=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_available_classes=['270492004','164889003', '164890007', '426627000','713427006', '713426002','445118002', '39732003','164909002', '251146004','698252002', '10370003','284470004','427172004','164947007', '111975006','164917005', '47665007','59118001', '427393009','426177001', '426783006','427084000', '63593006','164934002', '59931005', '17338001']\n",
    "path_G=\"/Study - ECG Atrial Fibrillation Prediction With Open Access Data/archive/G12ECG/WFDB/\"\n",
    "path_P=\"/Study - ECG Atrial Fibrillation Prediction With Open Access Data/archive/PTB_XL/WFDB/\"\n",
    "path_C=\"H:/CORAI_code_data/G_drive/Study - ECG Atrial Fibrillation Prediction With Open Access Data/archive/CPSC_Extra/\"\n",
    "positive_classes  = ['164889003', '164890007']\n",
    "dataset_paths = [path_C, path_G, path_P]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_metrics_df= pd.read_csv('/Study - ECG Atrial Fibrillation Prediction With Open Access Data/trained_models/Final_frame_to_work_withFilterSignalLengthCheck_new.csv')\n",
    "##as data saved in harddrive\n",
    "old_path = 'C:/Users/Admin/Downloads'\n",
    "#new_path = 'G:/Study - ECG Atrial Fibrillation Prediction With Open Access Data'\n",
    "new_path = '/Study - ECG Atrial Fibrillation Prediction With Open Access Data'\n",
    "\n",
    "calculated_metrics_df['Filename'] = calculated_metrics_df['Filename'].str.replace(old_path, new_path)\n",
    "##To handel the nan values: Only for HRV data, HRV features has nan values so remove them before training the model\n",
    "calculated_metrics_df = calculated_metrics_df.dropna()  # Drops rows with NaN values\n",
    "len(calculated_metrics_df)\n",
    "\n",
    "data_ecg_filenames=calculated_metrics_df['Filename']\n",
    "encoded_labels=calculated_metrics_df['Encoded_labels']\n",
    "age=calculated_metrics_df['Age']\n",
    "gender=calculated_metrics_df['Gender']\n",
    "HeartRate=calculated_metrics_df['HeartRate']\n",
    "InterBeatInterval=calculated_metrics_df['InterBeatInterval']\n",
    "HRV_SDNN=calculated_metrics_df['HRV_SDNN']\n",
    "HRV_RMSSD=calculated_metrics_df['HRV_RMSSD']\n",
    "PNN20=calculated_metrics_df['PNN20']\n",
    "PNN50=calculated_metrics_df['PNN50']\n",
    "HR_MAD=calculated_metrics_df['HR_MAD']\n",
    "Ratio_of_SD1_SD2=calculated_metrics_df['Ratio_of_SD1_SD2']\n",
    "InfoS=calculated_metrics_df['InfoS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataGenerator_all_demo_M(Sequence):\n",
    "    def __init__(self, filenames, labels=None, age_data=None, gender_data=None, HeartRate=None,InterBeatInterval=None,HRV_SDNN=None,HRV_RMSSD=None,HR_MAD=None,Ratio_of_SD1_SD2=None,batch_size=32, is_training=True):\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.age_data = age_data\n",
    "        self.gender_data = gender_data\n",
    "        self.HeartRate = HeartRate\n",
    "        self.InterBeatInterval= InterBeatInterval\n",
    "        self.HRV_SDNN = HRV_SDNN\n",
    "        self.HRV_RMSSD = HRV_RMSSD\n",
    "        self.HR_MAD = HR_MAD\n",
    "        self.Ratio_of_SD1_SD2 = Ratio_of_SD1_SD2\n",
    "        self.batch_size = batch_size\n",
    "        self.is_training = is_training\n",
    "\n",
    "        # Create a dictionary to map filenames to age and gender data\n",
    "        self.filename_to_data = {filename: (age, gender,HeartRate,InterBeatInterval,HRV_SDNN,HRV_RMSSD,HR_MAD,Ratio_of_SD1_SD2) for filename, age, gender,HeartRate,InterBeatInterval,HRV_SDNN,HRV_RMSSD,HR_MAD,Ratio_of_SD1_SD2 in zip(filenames, age_data, gender_data,HeartRate,InterBeatInterval,HRV_SDNN,HRV_RMSSD,HR_MAD,Ratio_of_SD1_SD2)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.filenames) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_filenames = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "\n",
    "        batch_demo_data = [self.filename_to_data[filename] for filename in batch_filenames]\n",
    "        #batch_demo_data = [demo_data for filename, demo_data in zip(batch_filenames, [self.filename_to_data[filename] for filename in batch_filenames])]\n",
    "\n",
    "        # Reshape demo_data to match the expected shape for inputB\n",
    "        batch_demo_data = np.array(batch_demo_data)\n",
    "\n",
    "        #batch_data = [self.load_data(filename, demo_data, label) for filename, demo_data, label in zip(batch_filenames, batch_demo_data, batch_labels)]\n",
    "        batch_data = [self.load_data(filename, demo_data, label) for filename, demo_data, label in zip(batch_filenames, batch_demo_data, self.labels[idx * self.batch_size:(idx + 1) * self.batch_size])]\n",
    "\n",
    "        # Assuming load_data returns a tuple of (input_data, target_data)\n",
    "        inputs, demo_data, targets = zip(*batch_data)\n",
    "\n",
    "        # Convert labels to one-hot encoded format\n",
    "        one_hot_targets = to_categorical(targets, num_classes=2)\n",
    "\n",
    "        # Modify to handle the demographic data correctly\n",
    "        if self.labels is not None:\n",
    "           # batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            #inputs, demo_data, targets = zip(*batch_data)\n",
    "            return [np.array(inputs), np.array(demo_data)], np.array(one_hot_targets)\n",
    "        else:\n",
    "            #inputs, demo_data= zip(*batch_data)\n",
    "            return [np.array(inputs), np.array(demo_data)], None\n",
    "\n",
    "    def load_data(self, filename, demo_data, label):\n",
    "        # Load data from the MAT file or any other required preprocessing\n",
    "        x = loadmat(filename)\n",
    "        data = np.asarray(x['val'], dtype=np.float64)\n",
    "        ecg_data =pad_sequences(data, maxlen=5000, truncating='post',padding=\"post\")\n",
    "        # Transpose the data to reshape it to (5000, 12)\n",
    "        ecg_data = np.transpose(ecg_data)\n",
    "\n",
    "        return ecg_data, demo_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_16_model_demo_HRV_M():\n",
    "\n",
    "    inputlayer = keras.layers.Input(shape=(5000,12)) \n",
    "    inputB = keras.layers.Input(shape=(8,))\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding='same',input_shape=(5000,12))(inputlayer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
    "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding='same')(conv1)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
    "    conv1 = keras.layers.MaxPool1D(pool_size=2, strides=2, padding='same')(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=128, kernel_size=3, padding='same')(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation('relu')(conv2)\n",
    "    conv2 = keras.layers.Conv1D(filters=128, kernel_size=3, padding='same')(conv2)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation('relu')(conv2)\n",
    "    conv2 = keras.layers.MaxPool1D(pool_size=2, strides=2, padding='same')(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=256, padding='same', kernel_size=3)(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation('relu')(conv3)\n",
    "    conv3 = keras.layers.Conv1D(filters=256, padding='same', kernel_size=3)(conv3)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation('relu')(conv3)\n",
    "    conv3 = keras.layers.Conv1D(filters=256, padding='same', kernel_size=3)(conv3)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation('relu')(conv3)\n",
    "    conv3 = keras.layers.MaxPool1D(pool_size=2, strides=2, padding='same')(conv3)\n",
    "\n",
    "    conv4 = keras.layers.Conv1D(filters=512, kernel_size=3, padding='same')(conv3)\n",
    "    conv4 = keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = keras.layers.Activation('relu')(conv4)\n",
    "    conv4 = keras.layers.Conv1D(filters=512, kernel_size=3, padding='same')(conv4)\n",
    "    conv4 = keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = keras.layers.Activation('relu')(conv4)\n",
    "    conv4 = keras.layers.Conv1D(filters=512, kernel_size=3, padding='same')(conv4)\n",
    "    conv4 = keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = keras.layers.Activation('relu')(conv4)\n",
    "    conv4 = keras.layers.MaxPool1D(pool_size=2, strides=2, padding='same')(conv4)\n",
    "\n",
    "    conv5 = keras.layers.Conv1D(filters=512, kernel_size=3, padding='same')(conv4)\n",
    "    conv5 = keras.layers.BatchNormalization()(conv5)\n",
    "    conv5 = keras.layers.Activation('relu')(conv5)\n",
    "    conv5 = keras.layers.Conv1D(filters=512, kernel_size=3, padding='same')(conv5)\n",
    "    conv5 = keras.layers.BatchNormalization()(conv5)\n",
    "    conv5 = keras.layers.Activation('relu')(conv5)\n",
    "    conv5 = keras.layers.Conv1D(filters=512, kernel_size=3, padding='same')(conv5)\n",
    "    conv5 = keras.layers.BatchNormalization()(conv5)\n",
    "    conv5 = keras.layers.Activation('relu')(conv5)\n",
    "    conv5 = keras.layers.MaxPool1D(pool_size=2, strides=2, padding='same')(conv5)\n",
    "    \n",
    "\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(conv5)\n",
    "    outputlayer1 = keras.layers.Dense(256, activation='relu')(gap_layer)\n",
    "    outputlayer1= keras.layers.Dropout(rate=0.3)(outputlayer1)\n",
    "    outputlayer2 = keras.layers.Dense(128, activation='relu')(outputlayer1)\n",
    "    outputlayer2= keras.layers.Dropout(rate=0.4)(outputlayer2)\n",
    "    \n",
    "    model1 = keras.Model(inputs=inputlayer, outputs=gap_layer)\n",
    "\n",
    "    mod3 = keras.layers.Dense(13, activation=\"relu\")(inputB) \n",
    "    mod3 = keras.layers.Dense(2, activation=\"softmax\")(mod3) \n",
    "    model3 = keras.Model(inputs=inputB, outputs=mod3)\n",
    "\n",
    "    combined = keras.layers.concatenate([model1.output, model3.output])\n",
    "    additional_layer = keras.layers.Dense(32, activation='relu')(combined)\n",
    "    additional_layer = keras.layers.Dropout(rate=0.3)(additional_layer)\n",
    "    additional_layer = keras.layers.Dense(16, activation='relu')(combined)\n",
    "    additional_layer = keras.layers.Dropout(rate=0.4)(additional_layer)\n",
    "    final_layer = keras.layers.Dense(2, activation=\"softmax\")(additional_layer)\n",
    "    model = keras.models.Model(inputs=[inputlayer,inputB], outputs=final_layer)\n",
    "\n",
    "\n",
    "    #outputlayer = keras.layers.Dense(1, activation='sigmoid')(outputlayer2)\n",
    "\n",
    "    #model = keras.Model(inputs=inputlayer, outputs=outputlayer)\n",
    "  \n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=[tf.keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n",
    "                    tf.keras.metrics.AUC(\n",
    "        num_thresholds=200,\n",
    "        curve=\"ROC\",\n",
    "        summation_method=\"interpolation\",\n",
    "        name=\"AUC\",\n",
    "        dtype=None,\n",
    "        thresholds=None,\n",
    "        label_weights=None,\n",
    "    )])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_AUC', factor=0.1, patience=1, verbose=1, mode='max',\n",
    "    min_delta=0.0001, cooldown=0, min_lr=0\n",
    ")\n",
    "base_dir_save=''\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "random_seed_list=[]\n",
    "\n",
    "for seed in range(len(random_seed_list)):\n",
    "    random_seed=random_seed_list[seed]\n",
    "    train_filenames, val_filenames, test_filenames, train_labels, val_labels, test_labels, train_age, val_age, test_age, train_gender, val_gender, test_gender = pc.split_and_shuffle_data(\n",
    "    data_ecg_filenames, encoded_labels, age, gender, random_state=random_seed, validation=True)\n",
    "\n",
    "    ##class-weights\n",
    "    from sklearn.utils import class_weight\n",
    "    class_weights = class_weight.compute_class_weight(class_weight='balanced',classes=np.unique(train_labels),y=train_labels)\n",
    "    class_weight=dict(zip(np.unique(train_labels),class_weights))\n",
    "\n",
    "    ##Model training\n",
    "    simplified_model_cw=pc.vgg_16_model_M()\n",
    "    simplified_model=pc.vgg_16_model_M()\n",
    "    batch_size = 128\n",
    "    train_generator = pc.ECGDataGenerator_M(train_filenames, train_labels,batch_size)\n",
    "    val_generator = pc.ECGDataGenerator_M(val_filenames, val_labels,batch_size)\n",
    "    history_cw=simplified_model_cw.fit(train_generator,epochs=100,steps_per_epoch=len(train_generator),validation_data=val_generator,validation_steps=len(val_generator),validation_freq=1, class_weight=class_weight,callbacks=[reduce_lr,early_stop])\n",
    "    history=simplified_model.fit(train_generator,epochs=100,steps_per_epoch=len(train_generator),validation_data=val_generator,validation_steps=len(val_generator),validation_freq=1,callbacks=[reduce_lr,early_stop])\n",
    "    ##model save\n",
    "    file_name_cw=os.path.join(base_dir_save,'VGG16_model_WCW{}.h5'.format(random_seed))\n",
    "    simplified_model_cw.save(file_name_cw)\n",
    "    file_name=os.path.join(base_dir_save,'VGG16_model_WoCW{}.h5'.format(random_seed))\n",
    "    simplified_model.save(file_name)\n",
    "    ##history save\n",
    "    history_file_cw = os.path.join(base_dir_save,'VGG16_model_WCW_history_{}.h5'.format(random_seed))\n",
    "    with open(history_file_cw, 'wb') as f:\n",
    "        pickle.dump(history_cw.history, f)   \n",
    "\n",
    "    history_file= os.path.join(base_dir_save,'VGG16_model_WoCW_history_{}.h5'.format(random_seed))\n",
    "    with open(history_file, 'wb') as f:\n",
    "        pickle.dump(history.history, f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_AUC', factor=0.1, patience=1, verbose=1, mode='max',\n",
    "    min_delta=0.0001, cooldown=0, min_lr=0\n",
    ")\n",
    "base_dir_save=''\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "\n",
    "random_seed_list=[]\n",
    "for seed in range(len(random_seed_list)):\n",
    "    random_seed=random_seed_list[seed]\n",
    "    train_filenames, val_filenames, test_filenames, train_labels, val_labels, test_labels, train_age, val_age, test_age, train_gender, val_gender, test_gender, train_HeartRate,val_HeartRate, test_HeartRate, train_InterBeatInterval,val_InterBeatInterval, test_InterBeatInterval, train_HRV_SDNN, val_HRV_SDNN, test_HRV_SDNN, train_HRV_RMSSD,val_HRV_RMSSD, test_HRV_RMSSD,train_HR_MAD,val_HR_MAD, test_HR_MAD, train_Ratio_of_SD1_SD2,val_Ratio_of_SD1_SD2, test_Ratio_of_SD1_SD2 = pc.split_and_shuffle_data_all(\n",
    "        data_ecg_filenames, encoded_labels, age, gender,HeartRate,InterBeatInterval, HRV_SDNN,HRV_RMSSD,HR_MAD,Ratio_of_SD1_SD2,random_state=random_seed, validation=True)\n",
    "\n",
    "    ##class-weights\n",
    "    from sklearn.utils import class_weight\n",
    "    class_weights = class_weight.compute_class_weight(class_weight='balanced',classes=np.unique(train_labels),y=train_labels)\n",
    "    class_weight=dict(zip(np.unique(train_labels),class_weights))\n",
    "\n",
    "    ##Model training\n",
    "    demo_model =pc.alexNet_model_HRV_M()\n",
    "    batch_size = 128\n",
    "    \n",
    "    train_generator = pc.ECGDataGenerator_all_HRV_M(train_filenames, train_labels, train_HeartRate,train_InterBeatInterval,train_HRV_SDNN,train_HRV_RMSSD,train_HR_MAD,train_Ratio_of_SD1_SD2, batch_size=batch_size)\n",
    "    val_generator = pc.ECGDataGenerator_all_HRV_M(val_filenames, val_labels,val_HeartRate,val_InterBeatInterval,val_HRV_SDNN,val_HRV_RMSSD,val_HR_MAD,val_Ratio_of_SD1_SD2,batch_size=batch_size)\n",
    "    history_demo=demo_model.fit(train_generator,epochs=100,steps_per_epoch=len(train_generator),validation_data=val_generator,validation_steps=len(val_generator),validation_freq=1,class_weight=class_weight,callbacks=[reduce_lr,early_stop])\n",
    "    file_name_demo=os.path.join(base_dir_save,'alexNet_HRV_WCW_model_{}.h5'.format(random_seed))\n",
    "    demo_model.save(file_name_demo)\n",
    "    history_file_demo = os.path.join(base_dir_save,'alexNet_HRV_WCW_model_history_{}.h5'.format(random_seed))\n",
    "    #history_file = 'history_RandSeed' + str(random_seed) + '.pkl'\n",
    "    with open(history_file_demo, 'wb') as f:\n",
    "        pickle.dump(history_demo.history, f)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_AUC', factor=0.1, patience=1, verbose=1, mode='max',\n",
    "    min_delta=0.0001, cooldown=0, min_lr=0\n",
    ")\n",
    "base_dir_save=''\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "\n",
    "random_seed_list=[]\n",
    "for seed in range(len(random_seed_list)):\n",
    "    random_seed=random_seed_list[seed]\n",
    "    train_filenames, val_filenames, test_filenames, train_labels, val_labels, test_labels, train_age, val_age, test_age, train_gender, val_gender, test_gender, train_HeartRate,val_HeartRate, test_HeartRate, train_InterBeatInterval,val_InterBeatInterval, test_InterBeatInterval, train_HRV_SDNN, val_HRV_SDNN, test_HRV_SDNN, train_HRV_RMSSD,val_HRV_RMSSD, test_HRV_RMSSD,train_HR_MAD,val_HR_MAD, test_HR_MAD, train_Ratio_of_SD1_SD2,val_Ratio_of_SD1_SD2, test_Ratio_of_SD1_SD2 = pc.split_and_shuffle_data_all(\n",
    "        data_ecg_filenames, encoded_labels, age, gender,HeartRate,InterBeatInterval, HRV_SDNN,HRV_RMSSD,HR_MAD,Ratio_of_SD1_SD2,random_state=random_seed, validation=True)\n",
    "\n",
    "    ##class-weights\n",
    "    from sklearn.utils import class_weight\n",
    "    class_weights = class_weight.compute_class_weight(class_weight='balanced',classes=np.unique(train_labels),y=train_labels)\n",
    "    class_weight=dict(zip(np.unique(train_labels),class_weights))\n",
    "\n",
    "    ##Model training\n",
    "    demo_model =pc.alexNet_model_demo_HRV_M()\n",
    "    batch_size = 128\n",
    "    \n",
    "    train_generator = pc.ECGDataGenerator_all_demo_M(train_filenames, train_labels, train_age, train_gender,train_HeartRate,train_InterBeatInterval,train_HRV_SDNN,train_HRV_RMSSD,train_HR_MAD,train_Ratio_of_SD1_SD2, batch_size=batch_size)\n",
    "    val_generator = pc.ECGDataGenerator_all_demo_M(val_filenames, val_labels, val_age, val_gender,val_HeartRate,val_InterBeatInterval,val_HRV_SDNN,val_HRV_RMSSD,val_HR_MAD,val_Ratio_of_SD1_SD2,batch_size=batch_size)\n",
    "    history_demo=demo_model.fit(train_generator,epochs=100,steps_per_epoch=len(train_generator),validation_data=val_generator,validation_steps=len(val_generator),validation_freq=1,class_weight=class_weight,callbacks=[reduce_lr,early_stop])\n",
    "    file_name_demo=os.path.join(base_dir_save,'alexNet_demo_HRV_WCW_model_{}.h5'.format(random_seed))\n",
    "    demo_model.save(file_name_demo)\n",
    "    history_file_demo = os.path.join(base_dir_save,'alexNet_demo_HRV_WCW_model_history_{}.h5'.format(random_seed))\n",
    "    #history_file = 'history_RandSeed' + str(random_seed) + '.pkl'\n",
    "    with open(history_file_demo, 'wb') as f:\n",
    "        pickle.dump(history_demo.history, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
